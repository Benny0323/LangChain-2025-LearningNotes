{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "è¿™ä¸ªç« èŠ‚å®˜æ–¹æ–‡æ¡£å¯ä»¥å‚è€ƒ[è¿™é‡Œ](https://python.langchain.com/docs/concepts/structured_outputs/)ï¼Œå‡†ç¡®æ¥è¯´æ˜¯å…³äº `Structured outputs` è€Œå¹¶ä¸å±€é™äº JSON Parser è¿™ä¸€ç±»å·¥å…·ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†æˆ‘ä»¬ä¼šæŒ‡å®šç‰¹æ®Šçš„ promptï¼Œè®© LLM èƒ½å¤Ÿè¿”å›ç»™æˆ‘ä»¬ç‰¹å®šç»“æ„çš„æ•°æ®ã€‚\n",
    "\n",
    "**ä¸ºä»€ä¹ˆé‡è¦å‘¢ï¼Ÿ** ä¸€èˆ¬çš„ LLM è¿”å›è¯­å¥éƒ½æ˜¯è´´è¿‘äººçš„è‡ªç„¶è¯­è¨€ï¼Œè€Œéå¯ä»¥ç»“æ„åŒ–å­˜å‚¨çš„æ–‡ä»¶ï¼ˆæ¯”å¦‚ JSONã€XML ç­‰ç­‰ï¼‰ã€‚å› æ­¤åœ¨å¸¸è§çš„ LLM åº”ç”¨å½“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šè®© LLM è¿”å›ç»“æ„åŒ–çš„æ•°æ®ï¼Œä»è€Œå‡å°‘äººä¸ºçš„æ•°æ®ç­›é€‰å’Œæ¸…æ´—ï¼Œæœ€å¤§åŒ–åˆ©ç”¨ LLM çš„ä¼˜åŠ¿ã€‚\n",
    "\n",
    "åœ¨å®˜æ–¹æ–‡æ¡£ä¸­è¿™å¼ å›¾è¡¨è¿°äº†ç»“æ„åŒ–è¾“å‡ºçš„æµç¨‹ï¼š\n",
    "![Returning structured output](https://python.langchain.com/assets/images/structured_output-2c42953cee807dedd6e96f3e1db17f69.png)\n",
    "\n",
    "å…³é”®ç‚¹åœ¨äºä¸¤ä¸ªï¼š\n",
    "1. æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›å®šä¹‰æ–¹å¼å°†è¾“å‡ºç»“æ„è¡¨ç¤ºä¸º `Schema`ï¼›\n",
    "2. ç»™å®š `Schema` ç»™ LLMï¼Œè®©å…¶è¾“å‡ºä¸ºè§„å®šçš„ç»“æ„åŒ–è¯­è¨€æ•°æ®ã€‚"
   ],
   "id": "adcd69661c1fff6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T23:06:18.799736Z",
     "start_time": "2025-04-05T23:06:17.909178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å®šä¹‰æˆ‘ä»¬å„é¡¹å‰ç½®å˜é‡\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,  # è®©æˆ‘ä»¬çš„æœºå™¨äººå˜å¾—ä¸¥è°¨ä¸”å‡†ç¡®\n",
    "    max_tokens=2048,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ],
   "id": "be3b6db7cddc3e8e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "æœ‰ä¸€ä¸ª Python åº“å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œç±»å‹æç¤ºå’ŒéªŒè¯ `pydantic`ã€‚",
   "id": "26c02d0448bf05b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T23:10:50.256264Z",
     "start_time": "2025-04-05T23:10:48.097547Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pydantic",
   "id": "67d6e4591db633bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.11/site-packages (2.10.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic) (2.27.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic) (4.12.2)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T23:11:07.142501Z",
     "start_time": "2025-04-05T23:11:07.106123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class ResponseFormatter(BaseModel):\n",
    "    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n",
    "    answer: str = Field(description=\"The answer to the user's question\")\n",
    "    followup_question: str = Field(description=\"A followup question the user could ask\")"
   ],
   "id": "2989897adf2b2fdd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥æå‰å¼•å…¥ tools çš„ç”¨æ³•ï¼Œæˆ‘ä»¬å°†åˆšåˆšå®šä¹‰çš„ `ResponseFormatter` ä¸æˆ‘ä»¬çš„ LLM ç»‘å®šåœ¨ä¸€èµ·ï¼Œä»è€Œè®© LLM æœ‰äº†ç»“æ„åŒ–è¾“å‡ºçš„èƒ½åŠ›ã€‚è¿™é‡Œæ˜¯ LangChain å…¼å®¹æ€§å¼ºçš„ä¸€ç‚¹ï¼Œèƒ½å¤Ÿä½¿ç”¨åŸç”Ÿæ¥å£ä¸ pydantic è¿›è¡Œç»‘å®šã€‚\n",
    "\n",
    "ä¸è¿‡å‡†ç¡®æ¥è¯´æ˜¯ `ChatOpenAI` è¿™ä¸ªç±»è‡ªå¸¦çš„ä¸€ç§å…¼å®¹èƒ½åŠ›ã€‚å¯ä»¥å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)çš„ Structure Output è¿™ä¸€æ ã€‚"
   ],
   "id": "d9eb21f592f0e3b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T23:17:06.861072Z",
     "start_time": "2025-04-05T23:17:04.296024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_with_tools = llm.bind_tools([ResponseFormatter])\n",
    "ai_msg = llm_with_tools.invoke(\"What is Low-Rank Decomposition?\")"
   ],
   "id": "8bbef7bcce8f891c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T23:20:27.602897Z",
     "start_time": "2025-04-05T23:20:27.592508Z"
    }
   },
   "cell_type": "code",
   "source": "print(ai_msg.tool_calls[0][\"args\"])",
   "id": "f54316c60479663f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Low-rank decomposition is a mathematical technique used to approximate a matrix by a product of two or more matrices with lower ranks. This method is particularly useful in data compression, noise reduction, and feature extraction. The idea is to represent a large matrix with a smaller set of data, capturing the most significant features while discarding less important information. Common methods for low-rank decomposition include Singular Value Decomposition (SVD), Principal Component Analysis (PCA), and Non-negative Matrix Factorization (NMF). These techniques are widely used in fields such as machine learning, statistics, and signal processing.', 'followup_question': 'How is Singular Value Decomposition (SVD) related to low-rank decomposition?'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# å¯ä»¥ä¼ å…¥ pydantic å‡½æ•°è¿›è¡ŒéªŒè¯å¹¶ä¸”è½¬æ¢ä¸º pydantic object\n",
    "pydantic_obj = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n",
    "print(pydantic_obj)\n",
    "# è¿™å°±æ˜¯åŸç”Ÿçš„è½¬æ¢æ–¹æ³•"
   ],
   "id": "6445cde7b802b14c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ LangChain è‡ªå¸¦çš„ `JSON Mode` æ–¹æ³•ï¼ˆç›¸æ¯”äºå‰è€…å¹¶æ²¡æœ‰ä½¿ç”¨åˆ° toolsï¼‰ã€‚å®é™…ä¸Šè¿™ç§æ¨¡å¼çš„ä¸»è¦æ–¹æ³•æ˜¯å¼ºåˆ¶æ¨¡å‹è¾“å‡ºåˆä¹è§„èŒƒçš„ JSONï¼ˆæ›´åƒæ˜¯ä¸éœ€è¦ä½ å£°æ˜çš„ Prompt Engineeringï¼‰ã€‚èƒ½å¤Ÿä½¿ç”¨ `JSON Mode` çš„æ¨¡å‹å·²ç»åœ¨[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/chat/#featured-providers)å†…åˆ—ä¸¾äº†å‡ºæ¥ã€‚ä½¿ç”¨æ–¹æ³•éå¸¸çš„ç®€å•ï¼Œåªéœ€è¦ä½¿ç”¨é“¾å¼å‡½æ•°åœ¨åé¢åŠ ä¸Š `.with_structured_output(method=\"json_mode\")` å³å¯ã€‚",
   "id": "919c4e9f6fac05a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T23:49:39.919494Z",
     "start_time": "2025-04-05T23:49:38.696227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_with_structure = llm.with_structured_output(method=\"json_mode\")\n",
    "ai_msg = llm_with_structure.invoke(\n",
    "    \"Return a JSON object of identities of 2 random person with key 'name', 'gender', 'height', 'weight', 'eye color', 'hair color'.\"\n",
    ")\n",
    "print(type(ai_msg))\n",
    "print(ai_msg)"
   ],
   "id": "d6c513e3df464d5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'person1': {'name': 'Alex Johnson', 'gender': 'Male', 'height': '180 cm', 'weight': '75 kg', 'eye color': 'Brown', 'hair color': 'Black'}, 'person2': {'name': 'Emily Smith', 'gender': 'Female', 'height': '165 cm', 'weight': '60 kg', 'eye color': 'Blue', 'hair color': 'Blonde'}}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ğŸ¤” Andrew Ng è¯¾ç¨‹ä¸Šä½¿ç”¨çš„æ–¹æ³•ä¸º `ResponseSchema` å’Œ `StructuredOutputParser`ï¼Œé¢„å…ˆå®šä¹‰ JSON ä¸­çš„ Key-Value å¯¹ï¼Œæœ‰ç‚¹ç±»ä¼¼äº ORM æ¡†æ¶ä¸­çš„æ˜ å°„å¯¹è±¡ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/structured/)æ‰¾åˆ°ã€‚ä½†æ˜¯å½“æ—¶çš„ç‰ˆæœ¬æ˜¯ v0.1ï¼Œç›®å‰æ–‡æ¡£å·²ç»æ ‡æ³¨äº†ä¸å†è¢«ç»´æŠ¤ï¼ŒåŒæ—¶ç³»ç»Ÿä¹Ÿä¼šæœ‰ `Deprecated` çš„è­¦å‘Šã€‚æˆ‘ä»¬å°±ä¸é‡‡ç”¨æ—§ç‰ˆæœ¬çš„å†™æ³•ï¼Œå…¨éƒ¨ä½¿ç”¨[æ–°ç‰ˆæœ¬æ–‡æ¡£](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method)ä¸­çš„å†™æ³•ã€‚\n",
    "\n",
    "æ“ä½œç²¾è®²ï¼šä¹‹å‰çš„è¯¾ç¨‹é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸‰ä¸ª `ResponseSchema`ï¼Œå¹¶ä½¿ç”¨ `StructuredOutputParser` å°†å…¶ç»„ç»‡èµ·æ¥ï¼Œä»è€Œè·å¾—äº†ä¸€ä¸ªç±»ä¼¼äº `Prompt Engineering` çš„æ“ä½œã€‚å…·ä½“çš„ä»£ç å¦‚ä¸‹ï¼š\n",
    "```python\n",
    "gift_schema = ResponseSchema(name=\"gift\", description=\"Was the item purchased as a gift for someone else?\")\n",
    "attitude_schema = ResponseSchema(name=\"attitude\", description=\"What is the attitude of this customer? Positive of Negative?\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\", description=\"What is the cost-effectiveness of this product?\")\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas([gift_schema, attitude_schema, price_value_schema])\n",
    "print(output_parser.get_format_instructions())\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬æ‰“å°å‡ºä¾æ¬¡å®šä¹‰çš„ schemaï¼Œå¯ä»¥å‘ç°å°±æ˜¯åœ¨æ¯ä¸ª instruction çš„æœ€ååŠ ä¸Šäº†äººä¸ºå®šä¹‰çš„ Promptï¼Œè¿™ä¹Ÿæ˜¯ `with_structured_output` çš„åŸç†ã€‚åªä¸è¿‡ç›®å‰ç‰ˆæœ¬åŒ…è£…å¾—éå¸¸å¤šï¼ŒæŠŠå¾ˆå¤šåº•å±‚çš„ Prompt éƒ½äººä¸ºå±è”½æ‰äº†ã€‚ä»¥ä¸‹å°±æ˜¯æ¯æ¬¡è¾“å…¥çš„æ—¶å€™é¢å¤–ä¼ å…¥çš„ Promptï¼š\n",
    "\n",
    "```text\n",
    "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "\\```json\n",
    "{\n",
    "\t\"gift\": string  // Was the item purchased as a gift for someone else?\n",
    "\t\"attitude\": string  // What is the attitude of this customer? Positive of Negative?\n",
    "\t\"price_value\": string  // What is the cost-effectiveness of this product?\n",
    "}\n",
    "\\```\n",
    "```"
   ],
   "id": "7e05b691eb20381d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OKï¼Œæˆ‘ä»¬æ¢³ç†ä¸€ä¸‹ç›®å‰çš„æ–¹æ³•ï¼š\n",
    "1. ä¸€ç§æ˜¯æ„å»ºä¸€ä¸ª pydantic æ ¼å¼å·¥å…·ï¼ˆåå­—ç±»ä¼¼äº `ResponseFormatter`ï¼‰ï¼Œç„¶åå°† LLM ç»‘å®šè¿™ä¸ªå·¥å…· `llm.bind_tools([ResponseFormatter])`ï¼Œæœ€åä½¿ç”¨ pydantic `ResponseFormatter.model_validate` çš„æ ¼å¼è½¬æ¢æ–¹æ³•å˜æˆæˆ‘ä»¬éœ€è¦çš„ JSON æ ¼å¼ï¼›\n",
    "2. å¦ä¸€ç§æ˜¯ç›´æ¥é’ˆå¯¹ LLM Providerï¼Œå¦‚æœå…è®¸ï¼Œä½¿ç”¨è‡ªå¸¦çš„ `JSON Mode` æ–¹æ³•ï¼Œæˆ‘ä»¬ `invoke` ä¹‹åæ¥æ”¶åˆ°çš„ `response` æœ¬èº«å°±ä¼šæˆä¸º dict ç±»å‹çš„æ•°æ®ï¼Œçº¦å®šäºæ˜¯ä¸€ä¸ª JSONï¼Œä½†æ˜¯ä¸ä¸€å®šæ‰€æœ‰çš„ Provider éƒ½ä¼šé€‚é…ä¸€ä¸ª `JSON Mode`ï¼ˆæ¯”å¦‚ [DeepSeek](https://python.langchain.com/docs/integrations/chat/deepseek/) å°±æ²¡æœ‰é€‚é…è¿™ä¸ª modeï¼‰ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯»æ±‚ä¸€ç§æ¯”è¾ƒå¥½çš„æ¨¡å¼ï¼Œå¯ä»¥è‡ªå·±å®šä¹‰ä¸€ä¸ª pydantic æ ¼å¼å·¥å…·ï¼Œç„¶åå°†å…¶ä½œä¸ºå‚æ•°ä¼ å…¥ LLM å½“ä¸­ï¼Œå¹¶ä¸”è¿”å›æ¥çš„å‚æ•°ç›´æ¥å°±æ˜¯ pydantic æ ¼å¼ï¼Œè¿™æ ·æˆ‘ä»¬ä¼šå°‘å¾ˆå¤šæ­¥éª¤ã€‚æœ€æ–°çš„å®˜æ–¹æ–‡æ¡£ä¸­å°±æä¾›äº†è¿™ä¸€ç§æ¨¡å¼ [Structure output method](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method)ï¼Œå‚è€ƒä¸‹å›¾çš„æµç¨‹ï¼š\n",
    "\n",
    "![Bind Schema to LLM](https://python.langchain.com/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)"
   ],
   "id": "e1b181d6959da62b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:03:22.600414Z",
     "start_time": "2025-04-06T00:03:20.366455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_with_structure = llm.with_structured_output(ResponseFormatter)\n",
    "structured_out = llm_with_structure.invoke(\"Which is bigger? 9.11 or 9.9?\")\n",
    "print(structured_out)"
   ],
   "id": "847a1a043a92c3c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='The number 9.9 is bigger than 9.11.\\n\\n### Explanation:\\n- **9.11** can be broken down as 9 + 0.11.\\n- **9.9** can be broken down as 9 + 0.9.\\n\\nWhen comparing the decimal parts:\\n- 0.11 is less than 0.9.\\n\\nTherefore, 9.9 is greater than 9.11.' followup_question='Would you like to know how to compare other decimal numbers?'\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:09:41.841733Z",
     "start_time": "2025-04-06T00:09:41.836814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(structured_out))\n",
    "# å°† ResponseFormatter åŒ…è£…çš„å›å¤å¼ºåˆ¶è½¬åŒ–æˆ JSON\n",
    "print(ResponseFormatter.model_validate(structured_out).model_dump_json())"
   ],
   "id": "9708d2efaf3f108f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.ResponseFormatter'>\n",
      "{\"answer\":\"The number 9.9 is bigger than 9.11.\\n\\n### Explanation:\\n- **9.11** can be broken down as 9 + 0.11.\\n- **9.9** can be broken down as 9 + 0.9.\\n\\nWhen comparing the decimal parts:\\n- 0.11 is less than 0.9.\\n\\nTherefore, 9.9 is greater than 9.11.\",\"followup_question\":\"Would you like to know how to compare other decimal numbers?\"}\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
