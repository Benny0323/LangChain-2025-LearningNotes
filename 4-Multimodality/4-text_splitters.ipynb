{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "文本分割器 [Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/)的作用在于将大文本分割成小文本，这样有几个好处：\n",
    "\n",
    "* 处理非均匀文档长度：现实世界的文档集合通常包含不同大小的文本。拆分可确保对所有文档进行一致的处理；\n",
    "* 克服模型限制：许多嵌入模型和语言模型都有最大输入大小限制。拆分使我们能够处理超出这些限制的文档；\n",
    "* 提高表示质量：对于较长的文档，嵌入或其他表示的质量可能会下降，因为它们试图捕获太多信息。拆分可以使每个部分的表示更加集中和准确；\n",
    "* 提高检索精度：在信息检索系统中，拆分可以提高搜索结果的粒度，使得查询与相关文档部分更精确地匹配；\n",
    "* 优化计算资源：处理较小的文本块可以更节省内存，并允许更好地并行化处理任务。\n",
    "\n",
    "可以在这个网页玩一玩 [ChunkViz](https://chunkviz.up.railway.app/)，一种可视化 splitter 的效果图，可以看到我们调整块大小，从而获得了不同的内容。"
   ],
   "id": "9cc2ced4106e16d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T17:03:21.665654Z",
     "start_time": "2025-04-08T17:03:21.610134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 我们仍旧使用 GPT-4o，这是一个支持多模态的 LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,  # 让我们的 LLM 最准确\n",
    "    max_tokens=2048,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ],
   "id": "f4fa17acb30176a2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LangChain 给我们提供了很多种方式进行文件的读取 [Document loaders](https://python.langchain.com/docs/how_to/#document-loaders)，包含着 PDF、web pages、CSV、JSON 等等。同时 LangChain 也提供了部分平台的数据接口，可以参考[官方文档](https://python.langchain.com/docs/integrations/document_loaders/)。",
   "id": "a7373d33110f754d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T17:40:59.005802Z",
     "start_time": "2025-04-08T17:40:58.958858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# 读取一整个\n",
    "docs = PyPDFLoader(\"./documents/ECE208_homework_tutorial.pdf\").load()\n",
    "texts = text_splitter.split_documents(\n",
    "    docs\n",
    ")\n",
    "\n",
    "for index, text in enumerate(texts):\n",
    "    print(f\"===================Chunk {index}====================\")\n",
    "    print(text.page_content) # 可以看到已经按照字符数进行了拆分"
   ],
   "id": "31c44dd700d4b620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Chunk 0====================\n",
      "ECE 208 Homework tutorial\n",
      "WI-22\n",
      "This is a short tutorial on how to use GitHub Classroom. It will cover some usage of GitHub\n",
      "commands, but we recommend you to refer to other materials for learning how to better use GitHub.\n",
      "1 Prepare\n",
      "Login to the your GitHub account.\n",
      "2 Access the homework\n",
      "• Go to the canvas Assignments page and click on the link in the homework.\n",
      "• Accept the assignment on the pop-up page.\n",
      "• You are ready to go! Refresh the page, and your assignment repository should have been created\n",
      "(this might take several minutes).\n",
      "• Click on the link on the GitHub Classroom page, and you will see the homework repository. It\n",
      "should be in this format:\n",
      "3 Work on the homework\n",
      "• Clone the homework repository:\n",
      "Open terminal and type:git clone $github repo\n",
      "For example,\n",
      "git clone\n",
      "Replace $id with the homework id (e.g. hw1) and$github user name with your GitHub user-\n",
      "name.\n",
      "• Feel free to use any Python IDE you are familiar or comfortable with.\n",
      "4 Submit the homework\n",
      "• Prepare the content staged for thecommit.\n",
      "Navigate to the homework repository in a terminal and stage the ﬁles for your submission using\n",
      "git add $name of the files\n",
      "Speciﬁcally, \"git add .\" will add all the ﬁles in the directory to the Index. By adding the\n",
      "ﬁles to the Git index, Git will start to track your added ﬁles.\n",
      "• Commit your changes\n",
      "Commit is like making a snapshot of the current state for everything in your stage area (Git\n",
      "index). Using git commit -m \"comments for this commit\" to commit your changes\n",
      "• Push your changes\n",
      "The above two steps modify our local copy of the repository, to add the changes to the repository\n",
      "in GitHub, use\"git push\"\n",
      "1\n",
      "===================Chunk 1====================\n",
      "5 Something to notice\n",
      "• Remember to check thestudent info.txt ﬁle in the repository and ﬁll in the information before\n",
      "you push your homework.This is important! Otherwise, your grading will not be recorded\n",
      "correctly.\n",
      "• Before you submit your homework, feel free to use theautocheck.py to check your homework.\n",
      "Run\n",
      "python autocheck.py\n",
      "Note that passing all the test cases in autocheck.py will not guarantee that your answer is correct.\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "我们尝试一下另外一种方法，使用 `RecursiveCharacterTextSplitter` 这一个方法更加智能。我们可以利用这种固有结构来指导我们的拆分策略，创建保持自然语言流的拆分，在拆分中保持语义连贯性，并适应不同级别的文本粒度。\n",
    "\n",
    "在使用 `CharacterTextSplitter(separator=\"\\n\", chunk_size=100)` 的时候，它会按换行 `\\n` 强制切割，然后截取固定长度的 chunk，不管语义是否完整。\n",
    "\n",
    "而使用 `RecursiveCharacterTextSplitter(chunk_size=100)` 的时候，它会保持较大的单元（例如段落）的完整性。如果一个单元超出了块大小，它就会移动到下一个级别（例如，句子）。如果有必要，这个过程会持续到单词级别。\n",
    "\n",
    "官方文档请查阅：[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)、[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)"
   ],
   "id": "1a2c5445f49eb36c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T17:40:10.981219Z",
     "start_time": "2025-04-08T17:40:10.948144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "doc = PyPDFLoader(\"./documents/ECE208_homework_tutorial.pdf\").load()\n",
    "texts = text_splitter.split_text(\n",
    "    doc[0].page_content\n",
    ")\n",
    "\n",
    "for index, text in enumerate(texts[0:]):\n",
    "    print(f\"===================Chunk {index}====================\")\n",
    "    print(text) # 可以看到已经按照字符数进行了拆分"
   ],
   "id": "b995d3b5cdc156d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Chunk 0====================\n",
      "ECE 208 Homework tutorial\n",
      "WI-22\n",
      "This is a short tutorial on how to use GitHub Classroom. It will cover some usage of GitHub\n",
      "commands, but we recommend you to refer to other materials for learning how to better use GitHub.\n",
      "1 Prepare\n",
      "Login to the your GitHub account.\n",
      "2 Access the homework\n",
      "• Go to the canvas Assignments page and click on the link in the homework.\n",
      "• Accept the assignment on the pop-up page.\n",
      "===================Chunk 1====================\n",
      "• You are ready to go! Refresh the page, and your assignment repository should have been created\n",
      "(this might take several minutes).\n",
      "• Click on the link on the GitHub Classroom page, and you will see the homework repository. It\n",
      "should be in this format:\n",
      "3 Work on the homework\n",
      "• Clone the homework repository:\n",
      "Open terminal and type:git clone $github repo\n",
      "For example,\n",
      "git clone\n",
      "===================Chunk 2====================\n",
      "Replace $id with the homework id (e.g. hw1) and$github user name with your GitHub user-\n",
      "name.\n",
      "• Feel free to use any Python IDE you are familiar or comfortable with.\n",
      "4 Submit the homework\n",
      "• Prepare the content staged for thecommit.\n",
      "Navigate to the homework repository in a terminal and stage the ﬁles for your submission using\n",
      "git add $name of the files\n",
      "===================Chunk 3====================\n",
      "Speciﬁcally, \"git add .\" will add all the ﬁles in the directory to the Index. By adding the\n",
      "ﬁles to the Git index, Git will start to track your added ﬁles.\n",
      "• Commit your changes\n",
      "Commit is like making a snapshot of the current state for everything in your stage area (Git\n",
      "index). Using git commit -m \"comments for this commit\" to commit your changes\n",
      "• Push your changes\n",
      "===================Chunk 4====================\n",
      "The above two steps modify our local copy of the repository, to add the changes to the repository\n",
      "in GitHub, use\"git push\"\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在上方的代码我们看不出什么明显的区别。我们使用中文来看一看：",
   "id": "d8b78edbb3336ff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T17:58:10.948271Z",
     "start_time": "2025-04-08T17:58:10.936078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"\n",
    "### 朴素贝叶斯的要求是什么？\n",
    "贝叶斯定理\n",
    "特征条件独立假设\n",
    "### 4. 朴素贝叶斯的优缺点？\n",
    "优点： 对小规模数据表现很好，适合多分类任务，适合增量式训练。\n",
    "缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。\n",
    "### 5. 朴素贝叶斯与LR区别？\n",
    "朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY)，最后利用贝叶斯定理求解P(Y|X)，而LR是判别模型，根据极大化对数似然函数直接求出条件概率P(Y|X)\n",
    "朴素贝叶斯是基于很强的**条件独立假设**（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求\n",
    "朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。\n",
    "\"\"\"\n",
    "doc = Document(page_content=text)"
   ],
   "id": "66a07d2b4ae8c0df",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T17:58:11.422665Z",
     "start_time": "2025-04-08T17:58:11.416467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用 CharacterTextSplitter（按换行切）\n",
    "char_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=50, chunk_overlap=0)\n",
    "char_chunks = char_splitter.split_documents([doc])\n",
    "\n",
    "# 使用 RecursiveCharacterTextSplitter（自动层级分割）\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "recursive_chunks = recursive_splitter.split_documents([doc])\n",
    "\n",
    "for index, text in enumerate(char_chunks):\n",
    "    print(f\"===================Char Splitter Chunk {index}====================\")\n",
    "    print(text) # 可以看到已经按照字符数进行了拆分\n",
    "\n",
    "print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "for index, text in enumerate(recursive_chunks):\n",
    "    print(f\"===================Recursive Splitter Chunk {index}====================\")\n",
    "    print(text) # 可以看到已经按照字符数进行了拆分"
   ],
   "id": "ca9d821528a59f84",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 115, which is longer than the specified 50\n",
      "Created a chunk of size 59, which is longer than the specified 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Char Splitter Chunk 0====================\n",
      "page_content='### 朴素贝叶斯的要求是什么？\n",
      "贝叶斯定理\n",
      "特征条件独立假设\n",
      "### 4. 朴素贝叶斯的优缺点？'\n",
      "===================Char Splitter Chunk 1====================\n",
      "page_content='优点： 对小规模数据表现很好，适合多分类任务，适合增量式训练。'\n",
      "===================Char Splitter Chunk 2====================\n",
      "page_content='缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。'\n",
      "===================Char Splitter Chunk 3====================\n",
      "page_content='### 5. 朴素贝叶斯与LR区别？'\n",
      "===================Char Splitter Chunk 4====================\n",
      "page_content='朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY)，最后利用贝叶斯定理求解P(Y|X)，而LR是判别模型，根据极大化对数似然函数直接求出条件概率P(Y|X)'\n",
      "===================Char Splitter Chunk 5====================\n",
      "page_content='朴素贝叶斯是基于很强的**条件独立假设**（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求'\n",
      "===================Char Splitter Chunk 6====================\n",
      "page_content='朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。'\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "===================Recursive Splitter Chunk 0====================\n",
      "page_content='### 朴素贝叶斯的要求是什么？\n",
      "贝叶斯定理\n",
      "特征条件独立假设\n",
      "### 4. 朴素贝叶斯的优缺点？'\n",
      "===================Recursive Splitter Chunk 1====================\n",
      "page_content='优点： 对小规模数据表现很好，适合多分类任务，适合增量式训练。'\n",
      "===================Recursive Splitter Chunk 2====================\n",
      "page_content='缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。'\n",
      "===================Recursive Splitter Chunk 3====================\n",
      "page_content='### 5. 朴素贝叶斯与LR区别？'\n",
      "===================Recursive Splitter Chunk 4====================\n",
      "page_content='朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而'\n",
      "===================Recursive Splitter Chunk 5====================\n",
      "page_content='求出联合分布概率P(XY)，最后利用贝叶斯定理求解P(Y|X)，而LR是判别模型，根据极大化对数似然'\n",
      "===================Recursive Splitter Chunk 6====================\n",
      "page_content='函数直接求出条件概率P(Y|X)'\n",
      "===================Recursive Splitter Chunk 7====================\n",
      "page_content='朴素贝叶斯是基于很强的**条件独立假设**（在已知分类Y的条件下，各个特征变量取值是相互独立的），'\n",
      "===================Recursive Splitter Chunk 8====================\n",
      "page_content='而LR则对此没有要求'\n",
      "===================Recursive Splitter Chunk 9====================\n",
      "page_content='朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。'\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " 实际上，`RecursiveCharacterTextSplitter` 是专门用来把 “大块文本智能切成小块” 的工具，它会尽量保留语义结构，在必要时再递归地切分。\n",
    "\n",
    "可以看到在 Char Splitter Chunk 4 中一段话有非常长的内容，因为优先级的问题，如果没有出现 `\\n`就不会切分，但是这样会让不同的文本长度跨越过大。但是在 Recursive Splitter Chunk 9 中就不会，根据提示 “Created a chunk of size 115, which is longer than the specified 50Created a chunk of size 115, which is longer than the specified 50”，实际上工具自动把我们的长度进行了递归的切分。"
   ],
   "id": "32bebf0c286ce35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在 LangChain 中还有别的的文本切分工具，比如 [Markdown](https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/)、[JSON](https://python.langchain.com/docs/how_to/recursive_json_splitter/)、[代码](https://python.langchain.com/docs/how_to/code_splitter/)、[HTML](https://python.langchain.com/docs/how_to/split_html/)，好在官方已经提供了对于这些文本的拆分器，具体使用方法其实是与 text splitter 相一致的。除此之外，还有一个更加美好的方法，是基于语义分割，参考 Greg Kamradt 的[文档](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)。",
   "id": "eaf4c5bfaeafd424"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
